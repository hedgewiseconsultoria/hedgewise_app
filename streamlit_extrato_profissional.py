import os
import streamlit as st
from huggingface_hub import InferenceClient

# ---------------------------------------------
# Configura√ß√£o do Streamlit
# ---------------------------------------------
st.set_page_config(page_title="Teste Llama 3 - Hugging Face", layout="centered")
st.title("ü¶ô Teste de Conex√£o com Llama 3.1 via Hugging Face")

# Token do Hugging Face
HF_TOKEN = os.getenv("HF_TOKEN")

if not HF_TOKEN:
    st.error("‚ùå Vari√°vel de ambiente HF_TOKEN n√£o encontrada. Configure o token no Secrets do Streamlit Cloud.")
    st.stop()

# Inicializa o cliente
try:
    client = InferenceClient(
        provider="featherless-ai",  # funciona apenas at√© huggingface_hub 0.20.3
        api_key=HF_TOKEN,
    )
    st.success("‚úÖ Cliente inicializado com sucesso.")
except Exception as e:
    st.error(f"Erro ao inicializar o cliente da Hugging Face: {e}")
    st.stop()

# Campo de texto para o prompt
prompt = st.text_area("Digite seu prompt para o Llama 3.1:", "Qual √© a capital da Fran√ßa?")

# Bot√£o para enviar √† IA
if st.button("üöÄ Enviar para o Llama 3.1"):
    st.info("Chamando o modelo Llama 3.1‚Ä¶ aguarde.")
    try:
        result = client.text_generation(
            prompt,
            model="meta-llama/Llama-3.1-8B",
            max_new_tokens=200,
            temperature=0.3,
        )
        st.subheader("üß© Resposta da IA:")
        st.write(result)
    except Exception as e:
        st.error(f"Erro ao chamar a API: {e}")
