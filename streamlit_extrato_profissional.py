import streamlit as st
import os
from huggingface_hub import InferenceClient

# -----------------------------
# Configura√ß√£o da p√°gina
# -----------------------------
st.set_page_config(page_title="Teste Llama 3.1", layout="centered")
st.title("üß† Teste de Conex√£o com Llama 3.1 (Hugging Face)")

# -----------------------------
# Token da Hugging Face (configure no Secrets do Streamlit Cloud)
# -----------------------------
HF_TOKEN = st.secrets.get("HF_TOKEN", None)

if not HF_TOKEN:
    st.error("‚ùå Token HF_TOKEN n√£o encontrado. Configure-o em 'Settings ‚Üí Secrets' no Streamlit Cloud.")
    st.stop()

# -----------------------------
# Inicializa o cliente Hugging Face
# -----------------------------
try:
    client = InferenceClient(
        model="meta-llama/Llama-3.1-8B",
        token=HF_TOKEN,
    )
    st.success("‚úÖ Conex√£o com o modelo Llama 3.1 inicializada com sucesso!")
except Exception as e:
    st.error(f"Erro ao inicializar o cliente da Hugging Face: {e}")
    st.stop()

# -----------------------------
# Entrada de texto do usu√°rio
# -----------------------------
prompt = st.text_area("Digite um prompt para testar:", "Explique o que √© intelig√™ncia artificial em poucas linhas.")

if st.button("Enviar para Llama 3.1 üöÄ"):
    with st.spinner("Gerando resposta..."):
        try:
            result = client.text_generation(
                prompt,
                max_new_tokens=400,
                temperature=0.7,
            )
            st.subheader("üßæ Resposta do modelo:")
            st.write(result)
        except Exception as e:
            st.error(f"Erro ao chamar a API da Hugging Face: {e}")


